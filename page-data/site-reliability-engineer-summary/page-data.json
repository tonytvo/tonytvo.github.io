{"componentChunkName":"component---src-templates-blog-post-js","path":"/site-reliability-engineer-summary/","result":{"data":{"site":{"siteMetadata":{"title":"Conversations on agile technical practices and investments","disqus":{"shortName":"trungvo"}}},"markdownRemark":{"id":"d17522ac-e42f-520f-9f00-2f784d267327","excerpt":"todo summarize devops handbook and site reliability engineering key takeaways introduction The book starts with a story about a time Margaret Hamilton brought…","html":"<h1 id=\"todo\" style=\"position:relative;\"><a href=\"#todo\" aria-label=\"todo permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>todo</h1>\n<ul>\n<li>summarize devops handbook and site reliability engineering</li>\n</ul>\n<h1 id=\"key-takeaways\" style=\"position:relative;\"><a href=\"#key-takeaways\" aria-label=\"key takeaways permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>key takeaways</h1>\n<h1 id=\"introduction\" style=\"position:relative;\"><a href=\"#introduction\" aria-label=\"introduction permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>introduction</h1>\n<p>The book starts with a story about a time Margaret Hamilton brought her young daughter with her to NASA, back in the days of the Apollo program. During a simulation mission, her daughter caused the mission to crash by pressing some keys that caused a prelaunch program to run during the simulated mission. Hamilton submitted a change request to add error checking code to prevent the error from happening again, but the request was rejected because the error case should never happen.</p>\n<p>On the next mission, Apollo 8, that exact error condition occurred and a potentially fatal problem that could have been prevented with a trivial check took NASA’s engineers 9 hours to resolve.</p>\n<ul>\n<li>Two approaches to hiring people to manage system stability:\n<ul>\n<li>\n<p><strong>Traditional approach: sysadmins</strong></p>\n<ul>\n<li>Assemble existing components and deploy to produce a service</li>\n<li>Respond to events and updates as they occur</li>\n<li>Grow team to absorb increased work as service grows</li>\n<li>Pros\n<ul>\n<li>Easy to implement because it’s standard</li>\n<li>Large talent pool to hire from</li>\n<li>Lots of available software</li>\n</ul>\n</li>\n<li>Cons\n<ul>\n<li>Manual intervention for change management and event handling causes size of team to scale with load on system</li>\n<li>Ops is fundamentally at odds with dev, which can cause pathological resistance to changes, which causes similarly pathological response from devs, which reclassify “launches” as “incremental updates”, “flag flips”, etc.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><strong>Google’s approach: SREs</strong></p>\n</li>\n<li>\n<p>Have software engineers do operations</p>\n</li>\n<li>\n<p>Candidates should be able to pass or nearly pass normal dev hiring bar, and may have some additional skills that are rare among devs (e.g., L1 - L3 networking or UNIX system internals).</p>\n</li>\n<li>\n<p>Career progress comparable to dev career track</p>\n</li>\n<li>\n<p>Results</p>\n</li>\n</ul>\nSREs would be bored by doing tasks by hand\nHave the skillset necessary to automate tasks\nDo the same work as an operations team, but with automation instead of manual labor\nTo avoid manual labor trap that causes team size to scale with service load, Google places a 50% cap on the amount of “ops” work for SREs\nUpper bound. Actual amount of ops work is expected to be much lower\nPros\nCheaper to scale\nCircumvents devs/ops split\nCons\nHard to hire for\nMay be unorthodox in ways that require management support (e.g., product team may push back against decision to stop releases for the quarter because the error budget is depleted)\nI don’t really understand how this is an example of circumventing the dev/ops split. I can see how it’s true in one sense, but the example of stopping all releases because an error budget got hit doesn’t seem fundamentally different from the “sysadmin” example where teams push back against launches. It seems that SREs have more political capital to spend and that, in the specific examples given, the SREs might be more reasonable, but there’s no reason to think that sysadmins can’t be reasonable.</li>\n</ul>\n<p>Tenets of SRE\nSRE team responsible for latency, performance, efficiency, change management, monitoring, emergency response, and capacity planning\nEnsuring a durable focus on engineering\n50% ops cap means that extra ops work is redirected to product teams on overflow\nProvides feedback mechanism to product teams as well as keeps load down\nTarget max 2 events per 8-12 hour on-call shift\nPostmortems for all serious incidents, even if they didn’t trigger a page\nBlameless postmortems\n2 events per shift is the max, but what’s the average? How many on-call events are expected to get sent from the SRE team to the dev team per week?</p>\n<p>How do you get from a blameful postmortem culture to a blameless postmortem culture? Now that everyone knows that you should have blameless postmortems, everyone will claim to do them. Sort of like having good testing and deployment practices. I’ve been lucky to be on an on call rotation that’s never gotten paged, but when I talk to folks who joined recently and are on call, they have not so great stories of finger pointing, trash talk, and blame shifting. The fact that everyone knows you’re supposed to be blameless seems to make it harder to call out blamefulness, not easier.</p>\n<p>Move fast without breaking SLO\nError budget. 100% is the wrong reliability target for basically everything\nGoing from 5 9s to 100% reliability isn’t noticeable to most users and requires tremendous effort\nSet a goal that acknowledges the trade-off and leaves an error budget\nError budget can be spent on anything: launching features, etc.\nError budget allows for discussion about how phased rollouts and 1% experiments can maintain tolerable levels of errors\nGoal of SRE team isn’t “zero outages” — SRE and product devs are incentive aligned to spend the error budget to get maximum feature velocity\nIt’s not explicitly stated, but for teams that need to “move fast”, consistently coming in way under the error budget could be taken as a sign that the team is spending too much effort on reliability.</p>\n<p>I like this idea a lot, but when I discussed this with Jessica Kerr, she pushed back on this idea because maybe you’re just under your error budget because you got lucky and a single really bad event can wipe out your error budget for the next decade. Followup question: how can you be confident enough in your risk model that you can purposefully consume error budget to move faster without worrying that a downstream (in time) bad event will put you overbudget? Nat Welch (a former Google SRE) responded to this by saying that you can build confidence through simulated disasters and other testing.</p>\n<p>Monitoring\nMonitoring should never require a human to interpret any part of the alerting domain\nThree valid kinds of monitoring output\nAlerts: human needs to take action immediately\nTickets: human needs to take action eventually\nLogging: no action needed\nNote that, for example, graphs are a type of log\nEmergency Response\nReliability is a function of MTTF (mean-time-to-failure) and MTTR (mean-time-to-recovery)\nFor evaluating responses, we care about MTTR\nHumans add latency\nSystems that don’t require humans to respond will have higher availability due to lower MTTR\nHaving a “playbook” produces 3x lower MTTR\nHaving hero generalists who can respond to everything works, but having playbooks works better\nI personally agree, but boy do we like our on call heros. I wonder how we can foster a culture of documentation.</p>\n<p>Change management\n70% of outages due to changes in a live system. Mitigation:\nImplement progressive rollouts\nMonitoring\nRollback\nRemove humans from the loop, avoid standard human problems on repetitive tasks\nDemand forecasting and capacity planning\nStraightforward, but a surprising number of teams/services don’t do it\nProvisioning\nAdding capacity riskier than load shifting, since it often involves spinning up new instances/locations, making significant changes to existing systems (config files, load balancers, etc.)\nExpensive enough that it should be done only when necessary; must be done quickly\nIf you don’t know what you actually need and overprovision that costs money\nEfficiency and performance\nLoad slows down systems\nSREs provision to meet capacity target with a specific response time goal\nEfficiency == money</p>\n<h1 id=\"embracing-risk\" style=\"position:relative;\"><a href=\"#embracing-risk\" aria-label=\"embracing risk permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Embracing risk</h1>\n<p>Ex: if a user is on a smartphone with 99% reliability, they can’t tell the difference between 99.99% and 99.999% reliability\nManaging risk\nReliability isn’t linear in cost. It can easily cost 100x more to get one additional increment of reliability\nCost associated with redundant equipment\nCost of building out features for reliability as opposed to “normal” features\nGoal: make systems reliable enough, but not too reliable!\nMeasuring service risk\nStandard practice: identify metric to represent property of system to optimize\nPossible metric = uptime / (uptime + downtime)\nProblematic for a globally distributed service. What does uptime really mean?\nAggregate availability = successful requests / total requests\nObv, not all requests are equal, but aggregate availability is an ok first order approximation\nUsually set quarterly targets\nRisk tolerance of services\nUsually not objectively obvious\nSREs work with product owners to translate business objectives into explicit objectives\nIdentifying risk tolerance of consumer services\nTODO: maybe read this in detail on second pass</p>\n<p>Identifying risk tolerance of infrastructure services\nTarget availability\nRunning ex: Bigtable\nSome consumer services serve data directly from Bigtable — need low latency and high reliability\nSome teams use bigtable as a backing store for offline analysis — care more about throughput than reliability\nToo expensive to meet all needs generically\nEx: Bigtable instance\nLow-latency Bigtable user wants low queue depth\nThroughput oriented Bigtable user wants moderate to high queue depth\nSuccess and failure are diametrically opposed in these two cases!\nCost\nPartition infra and offer different levels of service\nIn addition to obv. benefits, allows service to externalize the cost of providing different levels of service (e.g., expect latency oriented service to be more expensive than throughput oriented service)\nMotivation for error budgets\nNo notes on this because I already believe all of this. Maybe go back and re-read this if involved in debate about this.</p>\n<h1 id=\"service-level-objectives\" style=\"position:relative;\"><a href=\"#service-level-objectives\" aria-label=\"service level objectives permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Service level objectives</h1>\n<p>Note: skipping notes on terminology section.</p>\n<p>Ex: Chubby planned outages\nGoogle found that Chubby was consistently over its SLO, and that global Chubby outages would cause unusually bad outages at Google\nChubby was so reliable that teams were incorrectly assuming that it would never be down and failing to design systems that account for failures in Chubby\nSolution: take Chubby down globally when it’s too far above its SLO for a quarter to “show” teams that Chubby can go down\nWhat do you and your users care about?\nToo many indicators: hard to pay attention\nToo few indicators: might ignore important behavior\nDifferent classes of services should have different indicators\nUser-facing: availability, latency, throughput\nStorage: latency, availability, durability\nBig data: throughput, end-to-end latency\nAll systems care about correctness\nCollecting indicators\nCan often do naturally from server, but client-side metrics sometimes needed.\nAggregation\nUse distributions and not averages\nUser studies show that people usually prefer slower average with better tail latency\nStandardize on common defs, e.g., average over 1 minute, average over tasks in cluster, etc.\nCan have exceptions, but having reasonable defaults makes things easier\nChoosing targets\nDon’t pick target based on current performance\nCurrent performance may require heroic effort\nKeep it simple\nAvoid absolutes\nUnreasonable to talk about “infinite” scale or “always” available\nMinimize number of SLOs\nPerfection can wait\nCan always redefine SLOs over time\nSLOs set expectations\nKeep a safety margin (internal SLOs can be defined more loosely than external SLOs)\nDon’t overachieve\nSee Chubby example, above\nAnother example is making sure that the system isn’t too fast under light loads\nChapter 5: Eliminating toil\nCarla Geisser: “If a human operator needs to touch your system during normal operations, you have a bug. The definition of normal changes as your systems grow.”</p>\n<p>Def: Toil\nNot just “work I don’t want to do”\nManual\nRepetitive\nAutomatable\nTactical\nNo enduring value\nO(n) with service growth\nIn surveys, find 33% toil on average\nNumbers can be as low as 0% and as high as 80%\nToil > 50% is a sign that the manager should spread toil load more evenly\nIs toil always bad?\nPredictable and repetitive tasks can be calming\nCan produce a sense of accomplishment, can be low-risk / low-stress activities\nSection on why toil is bad. Skipping notetaking for that section.</p>\n<h1 id=\"monitoring-distributed-systems\" style=\"position:relative;\"><a href=\"#monitoring-distributed-systems\" aria-label=\"monitoring distributed systems permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Monitoring distributed systems</h1>\n<p>Why monitor?\nAnalyze long-term trends\nCompare over time or do experiments\nAlerting\nBuilding dashboards\nDebugging\nAs Alex Clemmer is wont to say, our problem isn’t that we move too slowly, it’s that we build the wrong thing. I wonder how we could get from where we are today to having enough instrumentation to be able to make informed decisions when building new systems.</p>\n<p>Setting reasonable expectations\nMonitoring is non-trivial\n10-12 person SRE team typically has 1-2 people building and maintaining monitoring\nNumber has decreased over time due to improvements in tooling/libs/centralized monitoring infra\nGeneral trend towards simpler/faster monitoring systems, with better tools for post hoc analysis\nAvoid “magic” systems\nLimited success with complex dependency hierarchies (e.g., “if DB slow, alert for DB, otherwise alert for website”).\nUsed mostly (only?) for very stable parts of system\nRules that generate alerts for humans should be simple to understand and represent a clear failure\nAvoiding magic includes avoiding ML?</p>\n<p>Lots of white-box monitoring\nSome black-box monitoring for critical stuff\nFour golden signals\nLatency\nTraffic\nErrors\nSaturation\nInteresting examples from Bigtable and Gmail from chapter not transcribed. A lot of information on the importance of keeping alerts simple also not transcribed.</p>\n<p>The long run\nThere’s often a tension between long-run and short-run availability\nCan sometimes fix unreliable systems through heroic effort, but that’s a burnout risk and also a failure risk\nTaking a controlled hit in short-term reliability is usually the better trade\nChapter 7: Evolution of automation at Google\n“Automation is a force multiplier, not a panacea”\nValue of automation\nConsistency\nExtensibility\nMTTR\nFaster non-repair actions\nTime savings\nMultiple interesting case studies and explanations skipped in notes.</p>\n<p>Chapter 8: Release engineering\nThis is a specific job function at Google\nRelease engineer role\nRelease engineers work with SWEs and SREs to define how software is released\nAllows dev teams to focus on dev work\nDefine best practices\nCompiler flags, formats for build ID tags, etc.\nReleases automated\nModels vary between teams\nCould be “push on green” and deploy every build\nCould be hourly builds and deploys\netc.\nHermetic builds\nBuilding same rev number should always give identical results\nSelf-contained — this includes versioning everything down the compiler used\nCan cherry-pick fixes against an old rev to fix production software\nVirtually all changes require code review\nBranching\nAll code in main branch\nReleases are branched off\nFixes can go from master to branch\nBranches never merged back\nTesting\nCI\nRelease process creates an audit trail that runs tests and shows that tests passed\nConfig management\nDeceptively simple, can cause instability\nMany possible schemes (all involve storing config in source control and having strict config review)\nUse mainline for config — config maintained at head and applied immediately\nOriginally used for Borg (and pre-Borg systems)\nBinary releases and config changes decoupled!\nInclude config files and binaries in same package\nSimple\nTightly couples binary and config — ok for projects with few config files or where few configs change\nPackage config into “configuration packages”\nSame hermetic principle as for code\nRelease engineering shouldn’t be an afterthought!\nBudget resources at beginning of dev cycle\nChapter 9: Simplicity\nStability vs. agility\nCan make things stable by freezing — need to balance the two\nReliable systems can increase agility\nReliable rollouts make it easier to link changes to bugs\nVirtue of boring!\nEssential vs. accidental complexity\nSREs should push back when accidental complexity is introduced\nCode is a liability\nRemove dead code or other bloat\nMinimal APIs\nSmaller APIs easier to test, more reliable\nModularity\nAPI versioning\nSame as code, where you’d avoid misc/util classes\nReleases\nSmall releases easier to measure\nCan’t tell what happened if we released 100 changes together\nChapter 10: Altering from time-series data\nBorgmon\nSimilar-ish to Prometheus\nCommon data format for logging\nData used for both dashboards and alerts\nFormalized a legacy data format, “varz”, which allowed metrics to be viewed via HTTP\nTo view metrics manually, go to <a href=\"http://foo:80/varz\">http://foo:80/varz</a>\nAdding a metric only requires a single declaration in code\nlow user-cost to add new metric\nBorgmon fetches /varz from each target periodically\nAlso includes synthetic data like health check, if name was resolved, etc.,\nTime series arena\nData stored in-memory, with checkpointing to disk\nFixed sized allocation\nGC expires oldest entries when full\nconceptually a 2-d array with time on one axis and items on the other axis\n24 bytes for a data point -> 1M unique time series for 12 hours at 1-minute intervals = 17 GB\nBorgmon rules\nAlgebraic expressions\nCompute time-series from other time-series\nRules evaluated in parallel on a threadpool\nCounters vs. gauges\nDef: counters are non-decreasing\nDef: can take any value\nCounters preferred to gauges because gauges can lose information depending on sampling interval\nAltering\nBorgmon rules can trigger alerts\nHave minimum duration to prevent “flapping”\nUsually set to two duration cycles so that missed collections don’t trigger an alert\nScaling\nBorgmon can take time-series data from other Borgmon (uses binary streaming protocol instead of the text-based varz protocol)\nCan have multiple tiers of filters\nProber\nBlack-box monitoring that monitors what the user sees\nCan be queried with varz or directly send alerts to Altertmanager\nConfiguration\nSeparation between definition of rules and targets being monitored\nChapter 11: Being on-call\nTypical response time\n5 min for user-facing or other time-critical tasks\n30 min for less time-sensitive stuff\nResponse times linked to SLOs\nEx: 99.99% for a quarter is 13 minutes of downtime; clearly can’t have response time above 13 minutes\nServices with looser SLOs can have response times in the 10s of minutes (or more?)\nPrimary vs secondary on-call\nWork distribution varies by team\nIn some, secondary can be backup for primary\nIn others, secondary handles non-urgent / non-paging events, primary handles pages\nBalanced on-call\nDef: quantity: percent of time on-call\nDef: quality: number of incidents that occur while on call\nThis is great. We should do this. People sometimes get really rough on-call rotations a few times in a row and considering the infrequency of on-call rotations there’s no reason to expect that this should randomly balance out over the course of a year or two.</p>\n<p>Balance in quantity</p>\n<blockquote>\n<p>= 50% of SRE time goes into engineering</p>\n</blockquote>\n<p>Of remainder, no more than 25% spent on-call\nPrefer multi-site teams\nNight shifts are bad for health, multi-site teams allow elimination of night shifts\nBalance in quality\nOn average, dealing with an incident (incl root-cause analysis, remediation, writing postmortem, fixing bug, etc.) takes 6 hours.\n=> shouldn’t have more than 2 incidents in a 12-hour on-call shift\nTo stay within upper bound, want very flat distribution of pages, with median value of 0\nCompensation — extra pay for being on-call (time-off or cash)\nChapter 12: Effective troubleshooting\nNo notes for this chapter.</p>\n<p>Chapter 13: Emergency response\nTest-induced emergency\nSREs break systems to see what happens\nEx: want to flush out hidden dependencies on a distributed MySQL database\nPlan: block access to 1/100 of DBs\nResponse: dependent services report that they’re unable to access key systems\nSRE response: SRE aborts exercise, tries to roll back permissions change\nRollback attempt fails\nAttempt to restore access to replicas works\nNormal operation restored in 1 hour\nWhat went well: dependent teams escalated issues immediately, were able to restore access\nWhat we learned: had an insufficient understanding of the system and its interaction with other systems, failed to follow incident response that would have informed customers of outage, hadn’t tested rollback procedures in test env\nChange-induced emergency\nChanges can cause failures!\nEx: config change to abuse prevention infra pushed on Friday triggered crash-loop bug\nAlmost all externally facing systems depend on this, become unavailable\nMany internal systems also have dependency and become unavailable\nAlerts start firing with seconds\nWithin 5 minutes of config push, engineer who pushed change rolled back change and services started recovering\nWhat went well: monitoring fired immediately, incident management worked well, out-of-band communications systems kept people up to date even though many systems were down, luck (engineer who pushed change was following real-time comms channels, which isn’t part of the release procedure)\nWhat we learned: push to canary didn’t trigger same issue because it didn’t hit a specific config keyword combination; push was considered low-risk and went through less stringent canary process, alerting was too noisy during outage\nProcess-induced emergency\nNo notes on process-induced example.</p>\n<p>Chapter 14: Managing incidents\nThis is an area where we seem to actually be pretty good. No notes on this chapter.</p>\n<p>Chapter 15: Postmortem culture: learning from failure\nI’m in strong agreement with most of this chapter. No notes.</p>\n<p>Chapter 16: Tracking outages\nEscalator: centralized system that tracks ACKs to alerts, notifies other people if necessary, etc.\nOutalator: gives time-interleaved view of notifications for multiple queues\nAlso saves related email and allows marking some messages as “important”, can collapse non-important messages, etc.\nOur version of Escalator seems fine. We could really use something like Outalator, though.</p>\n<p>Chapter 17: Testing for reliability\nPreaching to the choir. No notes on this section. We could really do a lot better here, though.</p>\n<p>Chapter 18: Software engineering in SRE\nEx: Auxon, capacity planning automation tool\nBackground: traditional capacity planning cycle</p>\n<ol>\n<li>collect demand forecasts (quarters to years in advance)</li>\n<li>Plan allocations</li>\n<li>Review plan</li>\n<li>Deploy and config resources</li>\n</ol>\n<p>Traditional approach cons\nMany things can affect plan: increase in efficiency, increase in adoption rate, cluster delivery date slips, etc.\nEven small changes require rechecking allocation plan\nLarge changes may require total rewrite of plan\nLabor intensive and error prone\nGoogle solution: intent-based capacity planning\nSpecify requirements, not implementation\nEncode requirements and autogenerate a capacity plan\nIn addition to saving labor, solvers can do better than human generated solutions => cost savings\nLadder of examples of increasingly intent based planning</p>\n<ol>\n<li>Want 50 cores in clusters X, Y, and Z — why those resources in those clusters?</li>\n<li>Want 50-core footprint in any 3 clusters in region — why that many resources and why 3?</li>\n<li>Want to meet demand with N+2 redundancy — why N+2?</li>\n<li>Want 5 9s of reliability. Could find, for example, that N+2 isn’t sufficient</li>\n</ol>\n<p>Found that greatest gains are from going to (3)\nSome sophisticated services may go for (4)\nPutting constraints into tools allows tradeoffs to be consistent across fleet\nAs opposed to making individual ad hoc decisions\nAuxon inputs\nRequirements (e.g., “service must be N+2 per continent”, “frontend servers no more than 50ms away from backend servers”\nDependencies\nBudget priorities\nPerformance data (how a service scales)\nDemand forecast data (note that services like Colossus have derived forecasts from dependent services)\nResource supply &#x26; pricing\nInputs go into solver (mixed-integer or linear programming solver)\nNo notes on why SRE software, how to spin up a group, etc. TODO: re-read back half of this chapter and take notes if it’s ever directly relevant for me.</p>\n<p>Chapter 19: Load balancing at the frontend\nNo notes on this section. Seems pretty similar to what we have in terms of high-level goals, and the chapter doesn’t go into low-level details. It’s notable that they do [redacted] differently from us, though. For more info on lower-level details, there’s the Maglev paper.</p>\n<p>Chapter 20: Load balancing in the datacenter\nFlow control\nNeed to avoid unhealthy tasks\nNaive flow control for unhealthy tasks\nTrack number of requests to a backend\nTreat backend as unhealthy when threshold is reached\nCons: generally terrible\nHealth-based flow control\nBackend task can be in one of three states: {healthy, refusing connections, lame duck}\nLame duck state can still take connections, but sends backpressure request to all clients\nLame duck state simplifies clean shutdown\nDef: subsetting: limiting pool of backend tasks that a client task can interact with\nClients in RPC system maintain pool of connections to backends\nUsing pool reduces latency compared to doing setup/teardown when needed\nInactive connections are relatively cheap, but not free, even in “inactive” mode (reduced health checks, UDP instead of TCP, etc.)\nChoosing the correct subset\nTyp: 20-100, choose base on workload\nSubset selection: random\nBad utilization\nSubset selection: round robin\nOrder is permuted; each round has its own permutation\nLoad balancing\nSubset selection is for connection balancing, but we still need to balance load\nLoad balancing: round robin\nIn practice, observe 2x difference between most loaded and least load\nIn practice, most expensive request can be 1000x more expensive than cheapest request\nIn addition, there’s random unpredictable variation in requests\nLoad balancing: least-loaded round robin\nExactly what it sounds like: round-robin among least loaded backends\nLoad appears to be measured in terms of connection count; may not always be the best metric\nThis is per client, not globally, so it’s possible to send requests to a backend with many requests from other clients\nIn practice, for larg services, find that most-loaded task uses twice as much CPU as least-loaded; similar to normal round robin\nLoad balancing: weighted round robin\nSame as above, but weight with other factors\nIn practice, much better load distribution than least-loaded round robin\nI wonder what Heroku meant when they responded to Rap Genius by saying “after extensive research and experimentation, we have yet to find either a theoretical model or a practical implementation that beats the simplicity and robustness of random routing to web backends that can support multiple concurrent connections”.</p>\n<p>Chapter 21: Handling overload\nEven with “good” load balancing, systems will become overloaded\nTypical strategy is to serve degraded responses, but under very high load that may not be possible\nModeling capacity as QPS or as a function of requests (e.g., how many keys the requests read) is failure prone\nThese generally change slowly, but can change rapidly (e.g., because of a single checkin)\nBetter solution: measure directly available resources\nCPU utilization is usually a good signal for provisioning\nWith GC, memory pressure turns into CPU utilization\nWith other systems, can provision other resources such that CPU is likely to be limiting factor\nIn cases where over-provisioning CPU is too expensive, take other resources into account\nHow much does it cost to generally over-provision CPU like that?</p>\n<p>Client-side throttling\nBackends start rejecting requests when customer hits quota\nRequests still use resources, even when rejected — without throttling, backends can spend most of their resources on rejecting requests\nCriticality\nSeems to be priority but with a different name?\nFirst-class notion in RPC system\nClient-side throttling keeps separate stats for each level of criticality\nBy default, criticality is propagated through subsequent RPCs\nHandling overloaded errors\nShed load to other DCs if DC is overloaded\nShed load to other backends if DC is ok but some backends are overloaded\nClients retry when they get an overloaded response\nPer-request retry budget (3)\nPer-client retry budget (10%)\nFailed retries from client cause “overloaded; don’t retry” response to be returned upstream\nHaving a “don’t retry” response is “obvious”, but relatively rare in practice. A lot of real systems have a problem with failed retries causing more retries up the stack. This is especially true when crossing a hardware/software boundary (e.g., filesystem read causes many retries on DVD/SSD/spinning disk, fails, and then gets retried at the filesystem level), but seems to be generally true in pure software too.</p>\n<p>Chapter 22: Addressing cascading failures\nTypical failure scenarios?\nServer overload\nEx: have two servers\nOne gets overloaded, failing\nOther one now gets all traffic and also fails\nResource exhaustion\nCPU/memory/threads/file descriptors/etc.\nEx: dependencies among resources</p>\n<ol>\n<li>Java frontend has poorly tuned GC params</li>\n<li>Frontend runs out of CPU due to GC</li>\n<li>CPU exhaustion slows down requests</li>\n<li>Increased queue depth uses more RAM</li>\n<li>Fixed memory allocation for entire frontend means that less memory is available for caching</li>\n<li>Lower hit rate</li>\n<li>More requests into backend</li>\n<li>Backend runs out of CPU or threads</li>\n<li>Health checks fail, starting cascading failure</li>\n</ol>\n<p>Difficult to determine cause during outage\nNote: policies that avoid servers that serve errors can make things worse\nfewer backends available, which get too many requests, which then become unavailable\nPreventing server overload\nLoad test! Must have realistic environment\nServe degraded results\nFail cheaply and early when overloaded\nHave higher-level systems reject requests (at reverse proxy, load balancer, and on task level)\nPerform capacity planning\nQueue management\nQueues do nothing in steady state\nQueued reqs consume memory and increase latency\nIf traffic is steady-ish, better to keep small queue size (say, 50% or less of thread pool size)\nEx: Gmail uses queueless servers with failover when threads are full\nFor bursty workloads, queue size should be function of #threads, time per req, size/freq of bursts\nSee also, adaptive LIFO and CoDel\nGraceful degradation\nNote that it’s important to test graceful degradation path, maybe by running a small set of servers near overload regularly, since this path is rarely exercised under normal circumstances\nBest to keep simple and easy to understand\nRetries\nAlways use randomized exponential backoff\nSee previous chapter on only retrying at a single level\nConsider having a server-wide retry budget\nDeadlines\nDon’t do work where deadline has been missed (common theme for cascading failure)\nAt each stage, check that deadline hasn’t been hit\nDeadlines should be propagated (e.g., even through RPCs)\nBimodal latency\nEx: problem with long deadline\nSay frontend has 10 servers, 100 threads each (1k threads of total cap)\nNormal operation: 1k QPS, reqs take 100ms => 100 worker threads occupied (1k QPS * .1s)\nSay 5% of operations don’t complete and there’s a 100s deadline\nThat consumes 5k threads (50 QPS * 100s)\nFrontend oversubscribed by 5x. Success rate = 1k / (5k + 95) = 19.6% => 80.4% error rate\nUsing deadlines instead of timeouts is great. We should really be more systematic about this.</p>\n<p>Not allowing systems to fill up with pointless zombie requests by setting reasonable deadlines is “obvious”, but a lot of real systems seem to have arbitrary timeouts at nice round human numbers (30s, 60s, 100s, etc.) instead of deadlines that are assigned with load/cascading failures in mind.</p>\n<p>Try to avoid intra-layer communication\nSimpler, avoids possible cascading failure paths\nTesting for cascading failures\nLoad test components!\nLoad testing both reveals breaking and point ferrets out components that will totally fall over under load\nMake sure to test each component separately\nTest non-critical backends (e.g., make sure that spelling suggestions for search don’t impede the critical path)\nImmediate steps to address cascading failures\nIncrease resources\nTemporarily stop health check failures/deaths\nRestart servers (only if that would help — e.g., in GC death spiral or deadlock)\nDrop traffic — drastic, last resort\nEnter degraded mode — requires having built this into service previously\nEliminate batch load\nEliminate bad traffic\nChapter 23: Distributed consensus for reliability\nHow do we agree on questions like…\nWhich process is the leader of a group of processes?\nWhat is the set of processes in a group?\nHas a message been successfully committed to a distributed queue?\nDoes a process hold a particular lease?\nWhat’s the value in a datastore for a particular key?\nEx1: split-brain\nService has replicated file servers in different racks\nMust avoid writing simultaneously to both file servers in a set to avoid data corruption\nEach pair of file servers has one leader &#x26; one follower\nServers monitor each other via heartbeats\nIf one server can’t contact the other, it sends a STONITH (shoot the other node in the head)\nBut what happens if the network is slow or packets get dropped?\nWhat happens if both servers issue STONITH?\nThis reminds me of one of my favorite distributed database postmortems. The database is configured as a ring, where each node talks to and replicates data into a “neighborhood” of 5 servers. If some machines in the neighborhood go down, other servers join the neighborhood and data gets replicated appropriately.</p>\n<p>Sounds good, but in the case where a server goes bad and decides that no data exists and all of its neighbors are bad, it can return results faster than any of its neighbors, as well as tell its neighbors that they’re all bad. Because the bad server has no data it’s very fast and can report that its neighbors are bad faster than its neighbors can report that it’s bad. Whoops!</p>\n<p>Ex2: failover requires human intervention\nA highly sharded DB has a primary for each shard, which replicates to a secondary in another DC\nExternal health checks decide if the primary should failover to its secondary\nIf the primary can’t see the secondary, it makes itself unavailable to avoid the problems from “Ex1”\nThis increases operational load\nProblems are correlated and this is relatively likely to run into problems when people are busy with other issues\nIf there’s a network issues, there’s no reason to think that a human will have a better view into the state of the world than machines in the system\nEx3: faulty group-membership algorithms\nWhat it sounds like. No notes on this part\nImpossibility results\nCAP: P is impossible in real networks, so choose C or A\nFLP: async distributed consensus can’t gaurantee progress with unreliable network\nPaxos\nSequence of proposals, which may or may not be accepted by the majority of processes\nNot accepted => fails\nSequence number per proposal, must be unique across system\nProposal\nProposer sends seq number to acceptors\nAcceptor agrees if it hasn’t seen a higher seq number\nProposers can try again with higher seq number\nIf proposer recvs agreement from majority, it commits by sending commit message with value\nAcceptors must journal to persistent storage when they accept\nPatterns\nDistributed consensus algorithms are a low-level primitive\nReliable replicated state machines\nFundamental building block for data config/storage, locking, leader election, etc.\nSee these papers: Schnieder, Aguilera, Amir &#x26; Kirsch\nReliable repliacted data and config stores\nNon distributed-consensus-based systems often use timestamps: problematic because clock synchrony can’t be gauranteed\nSee Spanner paper for an example of using distributed consensus\nLeader election\nEquivalent to distributed consensus\nWhere work of the leader can performed performed by one process or sharded, leader election pattern allows writing distributed system as if it were a simple program\nUsed by, for example, GFS and Colussus\nDistributed coordination and locking services\nBarrier used, for example, in MapReduce to make sure that Map is finished before Reduce proceeds\nDistributed queues and messaging\nQueues: can tolerate failures from worker nodes, but system needs to ensure that claimed tasks are processed\nCan use leases instead of removal from queue\nUsing RSM means that system can continue processing even when queue goes down\nPerformance\nConventional wisdom that consensus algorithms can’t be used for high-throughput low-latency systems is false\nDistributed consensus at the core of many Google systems\nScale makes this worse for Google than most other companies, but it still works\nMulti-Paxos\nStrong leader process: unless a leader has not yet been elected or a failure occurs, only one round trip required to reach consensus\nNote that another process in the group can propose at any time\nCan ping pong back and forth and pseudo-livelock\nNot unqique to multi-paxos,\nStandard solutions are to elect a proposer process or use rotating proposer\nScaling read-heavy workloads\nEx: Photon allows reads from any replica\nRead from stale replica requres extra work, but doesn’t produce bad incorrect results\nTo gaurantee reads are up to date, do one of the following:</p>\n<ol>\n<li>Perform a read-only consensus operation</li>\n<li>Read data from replica that’s guaranteed to be most-up-to-date (stable leader can provide this guarantee)</li>\n<li>Use quorum leases</li>\n</ol>\n<p>Quorum leases\nReplicas can be granted lease over some (or all) data in the system\nFast Paxos\nDesigned to be faster over WAN\nEach client can send Propose to each member of a group of acceptors directly, instead of through a leader\nNot necessarily faster than classic Paxos — if RTT to acceptors is long, we’ve traded one message across slow link plus N in parallel across fast link for N across slow link\nStable leaders\n“Almost all distributed consensus systems that have been designed with performance in mind use either the single stable leader pattern or a system of rotating leadership”\nTODO: finish this chapter?</p>\n<p>Chapter 24: Distributed cron\nTODO: go back and read in more detail, take notes.</p>\n<p>Chapter 25: Data processing pipelines\nExamples of this are MapReduce or Flume\nConvenient and easy to reason about the happy case, but fragile\nInitial install is usually ok because worker sizing, chunking, parameters are carefully tuned\nOver time, load changes, causes problems\nChapter 26: Data integrity\nDefinition not necessarily obvious\nIf an interface bug causes Gmail to fail to display messages, that’s the same as the data being gone from the user’s standpoint\n99.99% uptime means 1 hour of downtime per year. Probably ok for most apps\n99.99% good bytes in a 2GB file means 200K corrupt. Probably not ok for most apps\nBackup is non-trivial\nMay have mixture of transactional and non-transactional backup and restore\nDifferent versions of business logic might be live at once\nIf services are independently versioned, maybe have many combinations of versions\nReplicas aren’t sufficient — replicas may sync corruption\nStudy of 19 data recovery efforts at Google\nMost common user-visible data loss caused by deletion or loss of referential integrity due to software bugs\nHardest cases were low-grade corruption discovered weeks to months later\nDefense in depth\nFirst layer: soft deletion\nUsers should be able to delete their data\nBut that means that users will be able to accidentally delete their data\nAlso, account hijacking, etc.\nAccidentally deletion can also happen due to bugs\nSoft deletion delays actual deletion for some period of time\nSecond layer: backups\nNeed to figure out how much data it’s ok to lose during recovery, how long recovery can take, and how far back backups need to go\nWant backups to go back forever, since corruption can go unnoticed for months (or longer)\nBut changes to code and schema can make recovery of older backups expensive\nGoogle usually has 30 to 90 day window, depending on the service\nThird layer: early detection\nOut-of-band integrity checks\nHard to do this right!\nCorrect changes can cause checkers to fail\nBut loosening checks can cause failures to get missed\nNo notes on the two interesting case studies covered.</p>\n<p>Chapter 27: Reliable product launches at scale\nNo notes on this chapter in particular. A lot of this material is covered by or at least implied by material in other chapters. Probably worth at least looking at example checklist items and action items before thinking about launch strategy, though. Also see appendix E, launch coordination checklist.</p>\n<p>Chapters 28-32: Various chapters on management\nNo notes on these.</p>\n<p>Notes on the notes\nI like this book a lot. If you care about building reliable systems, reading through this book and seeing what the teams around you don’t do seems like a good exercise. That being said, the book isn’t perfect. The two big downsides for me stem from the same issue: this is one of those books that’s a collection of chapters by different people. Some of the editors are better than others, meaning that some of the chapters are clearer than others and that because the chapters seem designed to be readable as standalone chapters, there’s a fair amount of redundancy in the book if you just read it straight through. Depending on how you plan to use the book, that can be a positive, but it’s a negative to me. But even including he downsides, I’d say that this is the most valuable technical book I’ve read in the past year and I’ve covered probably 20% of the content in this set of notes. If you really like these notes, you’ll probably want to read the full book.</p>\n<p>If you found this set of notes way too dry, maybe try this much more entertaining set of notes on a totally different book. If you found this to only be slightly too dry, maybe try this set of notes on classes of errors commonly seen in postmortems. In any case, I’d appreciate feedback on these notes. Writing up notes is an experiment for me. If people find these useful, I’ll try to write up notes on books I read more often. If not, I might try a different approach to writing up notes or some other kind of post entirely.</p>\n<p>← Some programming blogs to consider reading\nWe only hire the trendiest →\nArchive\nMastodon\nThreads\nPatreon\nLinkedIn\nTwitter\nRSS</p>\n<h1 id=\"references\" style=\"position:relative;\"><a href=\"#references\" aria-label=\"references permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>References</h1>\n<ul>\n<li><a href=\"https://github.com/s905060/site-reliability-engineer-handbook\">https://github.com/s905060/site-reliability-engineer-handbook</a></li>\n<li><a href=\"https://dev.to/bitmaybewise/series/21343\">https://dev.to/bitmaybewise/series/21343</a></li>\n<li><a href=\"https://danluu.com/google-sre-book/\">https://danluu.com/google-sre-book/</a></li>\n<li><a href=\"https://kevinczarzasty.medium.com/summarizing-the-devops-handbook-how-to-create-world-class-agility-reliability-and-security-in-86d8357d9995\">https://kevinczarzasty.medium.com/summarizing-the-devops-handbook-how-to-create-world-class-agility-reliability-and-security-in-86d8357d9995</a></li>\n<li><a href=\"https://srinathramakrishnan.wordpress.com/wp-content/uploads/2017/02/the-devops-handbook-e28093-summary.pdf\">https://srinathramakrishnan.wordpress.com/wp-content/uploads/2017/02/the-devops-handbook-e28093-summary.pdf</a></li>\n</ul>","frontmatter":{"title":"site reliability engineering by Chris Jones summary - wip","date":"July 23, 2024","description":"site reliability engineering by Chris Jones summary"}},"previous":{"fields":{"slug":"/meditations-by-marcus-aurelius/"},"frontmatter":{"title":"meditations by Marcus Aurelius summary"}},"next":{"fields":{"slug":"/building-second-brain/"},"frontmatter":{"title":"building the second brain by Tiago Forte summary"}}},"pageContext":{"id":"d17522ac-e42f-520f-9f00-2f784d267327","previousPostId":"463f5fe2-8c41-5ea5-9351-3ce58e5cc26f","nextPostId":"9e8ecdfb-8c0a-58af-b3af-bb7a977dc292"}},"staticQueryHashes":["2841359383","3257411868"],"slicesMap":{}}